backend {
  default = slurm

  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        max-concurrent-workflows = 1
	# number of jobs
        concurrent-job-limit = 4
        runtime-attributes = """
	# number of cpus inside each job (short allows 40)
        Int cpus = 10
	# Memory for the job (short allows 30GB)
	Int mem = 30000
        String? docker
        """

        submit = """
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              ${"-c " + cpus} \
	      --mem ${mem}
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """
            # Ensure singularity is loaded if it's installed as a module
            # module load Singularity/3.0.1

            # Build the Docker image into a singularity image
            # DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
	    # echo $DOCKER_NAME
            # IMAGE=${cwd}/$DOCKER_NAME.sif
            # if [ ! -f $IMAGE ]; then
            #    singularity pull $IMAGE docker://${docker}
            # fi

            # Submit the script to SLURM
            sbatch \
              --wait \
              -J ${job_name} \
	      -p short \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              ${"-c " + cpus} \
              --mem=${mem} \
              --wrap "singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} bash ${docker_cwd}/execution/script"
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}

