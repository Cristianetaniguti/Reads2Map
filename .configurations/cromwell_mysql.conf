backend {
  default = Local

  providers {

    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        max-concurrent-workflows = 1
        concurrent-job-limit = 1

        filesystems {
          local {
            localization: [
              "hard-link", "soft-link", "copy"
            ]
          }
        }
      }
    }

    SlurmSingularity {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        filesystems {
           local {
             localization: [
                   "hard-link", "copy"
             ]
           }
        },
        max-concurrent-workflows = 1
        concurrent-job-limit = 5
        runtime-attributes = """
        String? time
        Int? cpu
        String? mem
        String? docker
        """

        submit = """
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              --time ${time} \
              --ntasks ${cpu} \
              --nodes=1 \
              --mem ${mem} \
              --wrap "/bin/bash ${script}"
        """

         submit-docker = """

            export SINGULARITY_CACHEDIR=$SCRATCH/.singularity
            export SINGULARITY_TMPDIR=$SCRATCH/temp

            module load WebProxy # load the Singularity

            # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
            # based on the users home.
            if [ -z $SINGULARITY_CACHEDIR ];
                then CACHE_DIR=$SCRATCH/.singularity
                else CACHE_DIR=$SINGULARITY_CACHEDIR
            fi

            # Build the Docker image into a singularity image
            DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})

            # The image will live together with all the other images to force "caching" of the .sif files themselves - note, always use docker hub tags!!!

            IMAGE=$SINGULARITY_CACHEDIR/$DOCKER_NAME.sif

            if [ ! -f $IMAGE ]; then  # If we already have the image, skip everything
                singularity pull $IMAGE docker://${docker}
            fi

            echo -e "#!/bin/sh\nsingularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}" > ${cwd}/script_${job_name}.sh

            # Submit the script to SLURM
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              --time ${time} \
              --ntasks=${cpu} \
              --nodes=1 \
              --mem=${mem} \
              --export=NONE \
              ${cwd}/script_${job_name}.sh
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }

    SGE {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        max-concurrent-workflows = 1
        concurrent-job-limit = 5
        runtime-attributes = """
        String? time
        Int? cpu
        String? mem
        String? docker
        """

        submit-docker = """
        # Ensure singularity is loaded if it's installed as a module
        module --ignore-cache load singularity
        export OMP_NUM_THREADS=1
        export MKL_NUM_THREADS=1
        export OMP_PLACES=threads
        export OMP_PROC_BIND=spread

        # Add here docker hub info if there are private repos

        # Submit the script to PBS
        qsub -A UQ-QAAFI \
          -l select=1:ncpus=${cpu}:mem=${mem} \
          -l walltime=${time} \
          bash -c "singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} bash ${docker_cwd}/execution/script"
        """

        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }

    SlurmDocker {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        max-concurrent-workflows = 1
	      # number of jobs
        concurrent-job-limit = 2
        runtime-attributes = """
          String? time
          Int? cpu
          String? mem
          String? docker
        """

        submit-docker = """
        # Submit the script to SLURM
        sbatch \
          -J ${job_name} \
          -D ${cwd} \
          -o ${cwd}/execution/stdout \
          -e ${cwd}/execution/stderr \
          --time ${time} \
          --ntasks ${cpu} \
          --nodes=1 \
          --mem ${mem} \
          --export=NONE \
          --wrap "docker run -v ${cwd}:${docker_cwd} ${docker} bash ${docker_cwd}/execution/script"
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}

database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://127.0.0.1:3306/cromwell?rewriteBatchedStatements=true&useSSL=false"
    user = "root"
    password = "1234"
    connectionTimeout = 50000
  }
}

call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}

docker {
    perform-registry-lookup-if-digest-is-provided = false
}
