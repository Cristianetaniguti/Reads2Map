backend {
  default = slurm

  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        filesystems {
           local {
             localization: [
                   "hard-link", "copy"
             ]
           }
        },

        system.io {
                  number-of-requests = 1
                  per = 1 seconds
                  number-of-attempts = 5
        }

        system {
               max-concurrent-workflows = 10
               max-workflow-launch-count = 1
               new-workflow-poll-rate = 50
        }

        concurrent-job-limit = 10

        runtime-attributes = """
        String? time
        String? tasks
        String? mem
        String? node
        String? docker
        """

               submit = """
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              --time ${time} \
              ${tasks} \
              ${node} \
              ${mem} \
              --mail-type=ALL \
              --mail-user=chtaniguti@tamu.edu \
              --wrap "/bin/bash ${script}"
        """
         # try sbatching up to 3 times every 30 second
         # solution by: https://github.com/ENCODE-DCC/caper/compare/v0.6.3...v0.6.4
         # some busy SLURM clusters spit out error, which results in a failure of the whole workflow
         submit-docker = """ITER=0; until [ $ITER -ge 3 ]; do
            export SINGULARITY_CACHEDIR=$SCRATCH/.singularity
            export SINGULARITY_TMPDIR=$SCRATCH/temp

            module load WebProxy # load the Singularity

            # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
            # based on the users home.
            if [ -z $SINGULARITY_CACHEDIR ];
                then CACHE_DIR=$SCRATCH/.singularity
                else CACHE_DIR=$SINGULARITY_CACHEDIR
            fi

            
            # Build the Docker image into a singularity image
            DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})

            # The image will live together with all the other images to force "caching" of the .sif files themselves - note, always use docker hub tags!!!

            IMAGE=$SINGULARITY_CACHEDIR/$DOCKER_NAME.sif

            if [ ! -f $IMAGE ]; then  # If we already have the image, skip everything
                singularity pull $IMAGE docker://${docker}
            fi

            echo -e "#!/bin/sh\nsingularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}" > ${cwd}/script_${job_name}.sh                                                  \


            # Submit the script to SLURM
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              --time ${time} \
              ${tasks} \
              ${node} \
              ${mem} \
              --export=NONE \
              --mail-type=ALL \
              --mail-user=chtaniguti@tamu.edu \
              ${cwd}/script_${job_name}.sh && break
         ITER=$[$ITER+1];sleep 30; done
        """

                kill = "scancel ${job_id}"

        # squeue every 30 second (up to 3 times)
        # unlike qstat -j JOB_ID, squeue -j JOB_ID doesn't return 1 when there is no such job
        # so we need to use squeue -j JOB_ID --noheader and check if output is empty
        # try polling up to 3 times since squeue fails on some busy SLURM clusters
        # e.g. on Stanford Sherlock, squeue didn't work when server is busy
        check-alive = """for ITER in 1 2 3; do CHK_ALIVE=$(squeue --noheader -j ${job_id} --format=%i | grep ${job_id}); if [ -z "$CHK_ALIVE" ]; then if [ "$ITER" == 3 ]; then /bin/bash -c 'exit 1'; else slee\
p 30; fi; else echo $CHK_ALIVE; break; fi; done"""
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}
